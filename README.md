# Optimizing-Deep-Neural-Networks-Initialization-Regularization-and-Gradient-Checking
This repository implements essential deep learning optimization techniques, focusing on:

- âœ… **Weight Initialization**
  - Zeros Initialization
  - Random (Large) Initialization
  - He Initialization

- âœ… **Regularization**
  - L2 Regularization
  - Dropout Regularization

- âœ… **Gradient Checking**
  - 1D Gradient Checking
  - N-Dimensional Gradient Checking

These implementations are based on neural networks using NumPy and cover:
- Improving convergence speed during training
- Preventing overfitting on small or noisy datasets
- Verifying the correctness of your backpropagation implementation

---

## ğŸ“ Folder Structure

.
â”œâ”€â”€ initialization/
â”‚ â”œâ”€â”€ initialize_parameters_zeros.py
â”‚ â”œâ”€â”€ initialize_parameters_random.py
â”‚ â””â”€â”€ initialize_parameters_he.py
â”‚
â”œâ”€â”€ regularization/
â”‚ â”œâ”€â”€ compute_cost_with_regularization.py
â”‚ â”œâ”€â”€ backward_propagation_with_regularization.py
â”‚ â””â”€â”€ dropout_forward_backward.py
â”‚
â”œâ”€â”€ gradient_checking/
â”‚ â”œâ”€â”€ gradient_check_1D.py
â”‚ â””â”€â”€ gradient_check_nD.py
â”‚
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ init_utils.py
â”‚ â”œâ”€â”€ reg_utils.py
â”‚ â””â”€â”€ gc_utils.py
â”‚
â”œâ”€â”€ testCases/
â”‚ â””â”€â”€ testCases.py
â”‚
â””â”€â”€ README.md
